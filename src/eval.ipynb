{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6cfeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/athul/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "\n",
    "run_dir = \"src/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8d018b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time domain model: time n_dims: 50\n",
      "Freq domain model: freq n_dims: 100\n"
     ]
    }
   ],
   "source": [
    "# Directly specify your model paths\n",
    "run_path_time = \"output/c6b14be6-5a5a-42c9-ad1d-504622d36218\"\n",
    "run_path_freq = \"output/c80936b9-3dd5-4e38-bb14-2501c4c34084\"\n",
    "\n",
    "# Load configs to verify\n",
    "_, conf_time = get_model_from_run(run_path_time, only_conf=True)\n",
    "_, conf_freq = get_model_from_run(run_path_freq, only_conf=True)\n",
    "\n",
    "print(\"Time domain model:\", conf_time.training.data_kwargs.domain, \"n_dims:\", conf_time.model.n_dims)\n",
    "print(\"Freq domain model:\", conf_freq.training.data_kwargs.domain, \"n_dims:\", conf_freq.model.n_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9980951",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m recompute_metrics = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recompute_metrics:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     metrics = \u001b[43mget_run_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/src/eval.py:6\u001b[39m, in \u001b[36mget_run_metrics\u001b[39m\u001b[34m(run_path, step, cache, skip_model_load, skip_baselines)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Munch\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/src/eval.py:28\u001b[39m, in \u001b[36mget_model_from_run\u001b[39m\u001b[34m(run_path, step, only_conf)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step == -\u001b[32m1\u001b[39m:\n\u001b[32m     27\u001b[39m     state_path = os.path.join(run_path, \u001b[33m\"\u001b[39m\u001b[33mstate.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     model.load_state_dict(state[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:1521\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:2122\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2120\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2121\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2122\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2123\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2125\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/_weights_only_unpickler.py:535\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    528\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    529\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    530\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m     ):\n\u001b[32m    532\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    533\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    537\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:2086\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2084\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2085\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2086\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:2052\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   2049\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.detect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m     wrap_storage = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2054\u001b[39m     storage._fake_device = location\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:698\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    680\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    700\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:636\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    634\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS C182/182-Final-Project/.venv/lib/python3.13/site-packages/torch/serialization.py:605\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    603\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    606\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    607\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    611\u001b[39m     )\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    613\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Choose which model to evaluate\n",
    "run_path = run_path_time  # or run_path_freq\n",
    "\n",
    "# Compute metrics (this will save to metrics.json in the run directory)\n",
    "recompute_metrics = True\n",
    "if recompute_metrics:\n",
    "    metrics = get_run_metrics(run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d09964",
   "metadata": {},
   "source": [
    "# Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e02c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": "# Load and plot metrics\nimport json\nwith open(os.path.join(run_path, \"metrics.json\")) as f:\n    metrics = json.load(f)\n\n_, conf = get_model_from_run(run_path, only_conf=True)\n# Don't filter by model names, just plot what's in metrics\nbasic_plot(metrics[\"standard\"])\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4ecca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Plot any OOD metrics (if available)\nfor name, metric in metrics.items():\n    if name == \"standard\": \n        continue\n    \n    basic_plot(metric)\n    plt.title(name)\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "e6f961d4",
   "metadata": {},
   "source": [
    "# Interactive setup\n",
    "\n",
    "We will now directly load the model and measure its in-context learning ability on a batch of random inputs. (In the paper we average over multiple such batches to obtain better estimates.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb327ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import get_data_sampler\n",
    "from tasks import get_task_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03523b06",
   "metadata": {},
   "outputs": [],
   "source": "model, conf = get_model_from_run(run_path)\n\n# Move model to appropriate device\nif torch.cuda.is_available():\n    model = model.cuda()\nelif torch.backends.mps.is_available():\n    model = model.to(\"mps\")\nmodel.eval()\n\nn_dims = conf.model.n_dims\nbatch_size = conf.training.batch_size\n\n# Override device for samplers\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\ndata_kwargs = dict(conf.training.data_kwargs)\ntask_kwargs = dict(conf.training.task_kwargs)\ndata_kwargs['device'] = device\ntask_kwargs['device'] = device\n\ndata_sampler = get_data_sampler(conf.training.data, n_dims, **data_kwargs)\ntask_sampler = get_task_sampler(\n    conf.training.task,\n    n_dims,\n    batch_size,\n    **task_kwargs\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9da7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task_sampler()\n",
    "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
    "ys = task.evaluate(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ddda",
   "metadata": {},
   "outputs": [],
   "source": "with torch.no_grad():\n    xs_device = xs.to(model.device if hasattr(model, 'device') else device)\n    ys_device = ys.to(model.device if hasattr(model, 'device') else device)\n    pred = model(xs_device, ys_device).cpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa97fa5",
   "metadata": {},
   "outputs": [],
   "source": "metric = task.get_metric()\n# Compute per-point MSE\npreds_np = pred.cpu().numpy()\nys_np = ys.cpu().numpy()\nloss = ((preds_np - ys_np) ** 2).mean(axis=(0, 2))  # mean over batch and features, keep n_points\n\n# Baseline for signal_conv: zero filter (output = 0)\nbaseline = 1.0\n\nplt.plot(loss, lw=2, label=\"Transformer\")\nplt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero filter\")\nplt.xlabel(\"# in-context examples\")\nplt.ylabel(\"squared error\")\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "eae775a1",
   "metadata": {},
   "source": "# Visualize example signals\n\nLet's plot some example input signals, their true outputs, and the model's predictions to see how well it learns the FIR filter."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e04e4",
   "metadata": {},
   "outputs": [],
   "source": "xs2 = 2 * xs\nys2 = task.evaluate(xs2)\nwith torch.no_grad():\n    xs2_device = xs2.to(model.device if hasattr(model, 'device') else device)\n    ys2_device = ys2.to(model.device if hasattr(model, 'device') else device)\n    pred2 = model(xs2_device, ys2_device).cpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71ba5",
   "metadata": {},
   "outputs": [],
   "source": "preds2_np = pred2.cpu().numpy()\nys2_np = ys2.cpu().numpy()\nloss2 = ((preds2_np - ys2_np) ** 2).mean(axis=(0, 2))\n\nplt.plot(loss, lw=2, label=\"Transformer\")\nplt.plot(loss2, lw=2, label=\"Transformer on doubled inputs\")\nplt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero filter\")\nplt.xlabel(\"# in-context examples\")\nplt.ylabel(\"squared error\")\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "code",
   "id": "021f118e",
   "metadata": {},
   "source": "# Plot example signals: input, true output, and predicted output\n# Pick one example from the batch\nexample_idx = 0\nn_points_to_show = 5  # Number of in-context examples to visualize\n\nfig, axes = plt.subplots(n_points_to_show, 3, figsize=(15, 3*n_points_to_show))\nif n_points_to_show == 1:\n    axes = axes.reshape(1, -1)\n\nfor point_idx in range(n_points_to_show):\n    # Get the signals for this point\n    x_signal = xs[example_idx, point_idx].cpu().numpy()\n    y_true = ys[example_idx, point_idx].cpu().numpy()\n    y_pred = pred[example_idx, point_idx].cpu().numpy()\n    \n    # Plot input signal\n    axes[point_idx, 0].plot(x_signal, 'b-', alpha=0.7)\n    axes[point_idx, 0].set_title(f'Input Signal (point {point_idx+1})')\n    axes[point_idx, 0].set_xlabel('Time' if conf.training.data_kwargs.domain == 'time' else 'Frequency bin')\n    axes[point_idx, 0].grid(True, alpha=0.3)\n    \n    # Plot true output\n    axes[point_idx, 1].plot(y_true, 'g-', alpha=0.7, label='True')\n    axes[point_idx, 1].set_title(f'True Output (point {point_idx+1})')\n    axes[point_idx, 1].set_xlabel('Time' if conf.training.task_kwargs.domain == 'time' else 'Frequency bin')\n    axes[point_idx, 1].grid(True, alpha=0.3)\n    \n    # Plot predicted output vs true\n    axes[point_idx, 2].plot(y_true, 'g-', alpha=0.7, label='True')\n    axes[point_idx, 2].plot(y_pred, 'r--', alpha=0.7, label='Predicted')\n    axes[point_idx, 2].set_title(f'Prediction vs True (point {point_idx+1})')\n    axes[point_idx, 2].set_xlabel('Time' if conf.training.task_kwargs.domain == 'time' else 'Frequency bin')\n    axes[point_idx, 2].legend()\n    axes[point_idx, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Show final point error\nfinal_error = ((y_pred - y_true) ** 2).mean()\nprint(f\"MSE for final point: {final_error:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc9cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}